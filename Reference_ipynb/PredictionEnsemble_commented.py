# ========================================
# ì¼€ê¸€ ê°€ìƒí™”í ê°€ê²©ì˜ˆì¸¡ ì•™ìƒë¸” ëª¨ë¸
# ========================================

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# !pip install koolbox scikit-learn==1.5.2

# ========================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ========================================
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
from lightgbm import LGBMRegressor
from scipy.stats import pearsonr as pr
from xgboost import XGBRegressor
from sklearn.base import clone
from koolbox import Trainer  # ì»¤ìŠ¤í…€ í›ˆë ¨ ìœ í‹¸ë¦¬í‹°
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
import joblib
import gc

warnings.filterwarnings("ignore")

# ========================================
# 2. ì„¤ì • íŒŒë¼ë¯¸í„°
# ========================================
class CFG:
    """ì‹¤í—˜ ì„¤ì • í´ë˜ìŠ¤"""
    train_path = "../data/drw-crypto-market-prediction/train.parquet"
    test_path = "../data/drw-crypto-market-prediction/test.parquet"
    sample_sub_path = "../data/drw-crypto-market-prediction/sample_submission.csv"
    
    target = "label"  # ì˜ˆì¸¡í•  íƒ€ê²Ÿ ë³€ìˆ˜
    n_folds = 5  # êµì°¨ê²€ì¦ í´ë“œ ìˆ˜
    seed = 42  # ëœë¤ ì‹œë“œ
    
    run_optuna = True  # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ ì—¬ë¶€
    n_optuna_trials = 250  # Optuna ì‹œí–‰ íšŸìˆ˜

# ========================================
# 3. ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜
# ========================================
def reduce_mem_usage(dataframe, dataset):    
    """ë°ì´í„°í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        dataframe: ìµœì í™”í•  ë°ì´í„°í”„ë ˆì„
        dataset: ë°ì´í„°ì…‹ ì´ë¦„ (ë¡œê¹…ìš©)
    
    Returns:
        ìµœì í™”ëœ ë°ì´í„°í”„ë ˆì„
    """
    print('Reducing memory usage for:', dataset)
    initial_mem_usage = dataframe.memory_usage().sum() / 1024**2
    
    for col in dataframe.columns:
        col_type = dataframe[col].dtype
        c_min = dataframe[col].min()
        c_max = dataframe[col].max()
        
        # ì •ìˆ˜í˜• ë°ì´í„° íƒ€ì… ìµœì í™”
        if str(col_type)[:3] == 'int':
            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                dataframe[col] = dataframe[col].astype(np.int8)
            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                dataframe[col] = dataframe[col].astype(np.int16)
            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                dataframe[col] = dataframe[col].astype(np.int32)
            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                dataframe[col] = dataframe[col].astype(np.int64)
        # ì‹¤ìˆ˜í˜• ë°ì´í„° íƒ€ì… ìµœì í™”
        else:
            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                dataframe[col] = dataframe[col].astype(np.float16)
            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                dataframe[col] = dataframe[col].astype(np.float32)
            else:
                dataframe[col] = dataframe[col].astype(np.float64)

    final_mem_usage = dataframe.memory_usage().sum() / 1024**2
    print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))
    print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))
    print('--- Decreased memory usage by {:.1f}%\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))

    return dataframe

# ========================================
# 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜
# ========================================
def add_features(df):
    """ê°€ìƒí™”í ê±°ë˜ ë°ì´í„°ì—ì„œ ê¸ˆìœµ ì§€í‘œ í”¼ì²˜ë“¤ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df: ì›ë³¸ ê±°ë˜ ë°ì´í„°
    
    Returns:
        ìƒì„±ëœ í”¼ì²˜ë“¤ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„
    """
    data = df.copy()
    features_df = pd.DataFrame(index=data.index)
    
    # === í˜¸ê°€ì°½ ê´€ë ¨ ì§€í‘œ ===
    features_df['bid_ask_spread_proxy'] = data['ask_qty'] - data['bid_qty']  # ë§¤ë„-ë§¤ìˆ˜ í˜¸ê°€ ì°¨ì´
    features_df['total_liquidity'] = data['bid_qty'] + data['ask_qty']  # ì „ì²´ ìœ ë™ì„±
    features_df['order_imbalance'] = (data['bid_qty'] - data['ask_qty']) / (data['bid_qty'] + data['ask_qty'] + 1e-8)  # ì£¼ë¬¸ ë¶ˆê· í˜•
    features_df['order_imbalance_abs'] = np.abs(features_df['order_imbalance'])  # ì£¼ë¬¸ ë¶ˆê· í˜• ì ˆëŒ“ê°’
    
    # === ê±°ë˜ëŸ‰ ê´€ë ¨ ì§€í‘œ ===
    features_df['trade_imbalance'] = data['buy_qty'] - data['sell_qty']  # ë§¤ìˆ˜-ë§¤ë„ ê±°ë˜ëŸ‰ ì°¨ì´
    features_df['total_trades'] = data['buy_qty'] + data['sell_qty']  # ì´ ê±°ë˜ëŸ‰
    features_df['volume_per_trade'] = data['volume'] / (data['buy_qty'] + data['sell_qty'] + 1e-8)  # ê±°ë˜ë‹¹ í‰ê·  ê±°ë˜ëŸ‰
    
    # === ë§¤ìˆ˜/ë§¤ë„ ì••ë ¥ ì§€í‘œ ===
    features_df['buying_pressure'] = data['buy_qty'] / (data['buy_qty'] + data['sell_qty'] + 1e-8)  # ë§¤ìˆ˜ ì••ë ¥
    features_df['selling_pressure'] = data['sell_qty'] / (data['buy_qty'] + data['sell_qty'] + 1e-8)  # ë§¤ë„ ì••ë ¥
    features_df['buy_volume_ratio'] = data['buy_qty'] / (data['volume'] + 1e-8)  # ë§¤ìˆ˜ ê±°ë˜ëŸ‰ ë¹„ìœ¨
    features_df['sell_volume_ratio'] = data['sell_qty'] / (data['volume'] + 1e-8)  # ë§¤ë„ ê±°ë˜ëŸ‰ ë¹„ìœ¨
    
    # === ìœ ë™ì„± ê´€ë ¨ ì§€í‘œ ===
    features_df['bid_liquidity_ratio'] = data['bid_qty'] / (data['volume'] + 1e-8)  # ë§¤ìˆ˜ í˜¸ê°€ ìœ ë™ì„± ë¹„ìœ¨
    features_df['ask_liquidity_ratio'] = data['ask_qty'] / (data['volume'] + 1e-8)  # ë§¤ë„ í˜¸ê°€ ìœ ë™ì„± ë¹„ìœ¨
    features_df['volume_liquidity_ratio'] = data['volume'] / (data['bid_qty'] + data['ask_qty'] + 1e-8)  # ê±°ë˜ëŸ‰ ëŒ€ë¹„ ìœ ë™ì„±
    
    # === ë¹„ìœ¨ ì§€í‘œ ===
    features_df['buy_sell_ratio'] = data['buy_qty'] / (data['sell_qty'] + 1e-8)  # ë§¤ìˆ˜/ë§¤ë„ ë¹„ìœ¨
    features_df['bid_ask_ratio'] = data['bid_qty'] / (data['ask_qty'] + 1e-8)  # ë§¤ìˆ˜í˜¸ê°€/ë§¤ë„í˜¸ê°€ ë¹„ìœ¨
    
    # === ìƒí˜¸ì‘ìš© ì§€í‘œ ===
    features_df['buy_volume_product'] = data['buy_qty'] * data['volume']  # ë§¤ìˆ˜ëŸ‰ê³¼ ê±°ë˜ëŸ‰ì˜ ê³±
    features_df['sell_volume_product'] = data['sell_qty'] * data['volume']  # ë§¤ë„ëŸ‰ê³¼ ê±°ë˜ëŸ‰ì˜ ê³±
    features_df['bid_ask_product'] = data['bid_qty'] * data['ask_qty']  # í˜¸ê°€ëŸ‰ë“¤ì˜ ê³±
    
    # === ì‹œì¥ ê²½ìŸë„ ì§€í‘œ ===
    features_df['market_competition'] = (data['buy_qty'] * data['sell_qty']) / ((data['buy_qty'] + data['sell_qty']) + 1e-8)  # ì‹œì¥ ê²½ìŸë„
    features_df['liquidity_competition'] = (data['bid_qty'] * data['ask_qty']) / ((data['bid_qty'] + data['ask_qty']) + 1e-8)  # ìœ ë™ì„± ê²½ìŸë„
    
    # === ì‹œì¥ í™œë™ë„ ì§€í‘œ ===
    total_activity = data['buy_qty'] + data['sell_qty'] + data['bid_qty'] + data['ask_qty']
    features_df['market_activity'] = total_activity  # ì „ì²´ ì‹œì¥ í™œë™ë„
    features_df['activity_concentration'] = data['volume'] / (total_activity + 1e-8)  # í™œë™ ì§‘ì¤‘ë„
    features_df['depth_imbalance'] = features_df['total_trades'] - data['volume']  # ê¹Šì´ ë¶ˆê· í˜•
    
    # === ì •ë³´ íë¦„ ì§€í‘œ ===
    features_df['info_arrival_rate'] = (data['buy_qty'] + data['sell_qty']) / (data['volume'] + 1e-8)  # ì •ë³´ ë„ì°©ë¥ 
    features_df['market_making_intensity'] = (data['bid_qty'] + data['ask_qty']) / (data['buy_qty'] + data['sell_qty'] + 1e-8)  # ë§ˆì¼“ë©”ì´í‚¹ ê°•ë„
    features_df['effective_spread_proxy'] = np.abs(data['buy_qty'] - data['sell_qty']) / (data['volume'] + 1e-8)  # ìœ íš¨ ìŠ¤í”„ë ˆë“œ ê·¼ì‚¬ì¹˜
    
    # === ì‹œê³„ì—´ ì§€í‘œ (ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· ) ===
    lambda_decay = 0.95  # ê°ì‡  ê³„ìˆ˜
    ofi = data['buy_qty'] - data['sell_qty']  # Order Flow Imbalance
    features_df['order_flow_imbalance_ewm'] = ofi.ewm(alpha=1-lambda_decay).mean()  # OFI ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· 

    # ë¬´í•œê°’ê³¼ NaN ì²˜ë¦¬
    features_df = features_df.replace([np.inf, -np.inf], np.nan)
    
    return features_df

# ========================================
# 5. ì œê±°í•  íŠ¹ì„± ëª©ë¡ (ë…¸ì´ì¦ˆê°€ ë§ì€ íŠ¹ì„±ë“¤)
# ========================================
cols_to_drop = [
    'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 
    'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716',
    'X717', 'X864', 'X867', 'X869', 'X870', 'X871', 'X872', 'X104', 'X110', 'X116',
    'X122', 'X128', 'X134', 'X140', 'X146', 'X152', 'X158', 'X164', 'X170', 'X176',
    'X182', 'X351', 'X357', 'X363', 'X369', 'X375', 'X381', 'X387', 'X393', 'X399',
    'X405', 'X411', 'X417', 'X423', 'X429'
]

# ========================================
# 6. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# ========================================
print("ë°ì´í„° ë¡œë”© ì¤‘...")
train = pd.read_parquet(CFG.train_path).reset_index(drop=True)
test = pd.read_parquet(CFG.test_path).reset_index(drop=True)

# ë©”ëª¨ë¦¬ ìµœì í™”
train = reduce_mem_usage(train, "train")
test = reduce_mem_usage(test, "test")

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = train.drop([CFG.target] + cols_to_drop, axis=1)
y = train[CFG.target]
X_test = test.drop([CFG.target] + cols_to_drop, axis=1)

# ìƒˆë¡œìš´ í”¼ì²˜ë“¤ì„ ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
print("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì§„í–‰ ì¤‘...")
X = pd.concat([add_features(X), X], axis=1)
X_test = pd.concat([add_features(X_test), X_test], axis=1)

# ========================================
# 7. í‰ê°€ ì§€í‘œ í•¨ìˆ˜
# ========================================
def pearsonr(y_true, y_pred):
    """í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (í‰ê°€ ì§€í‘œ)"""
    return pr(y_true, y_pred)[0]

# ========================================
# 8. ë¯¸ë¦¬ íŠœë‹ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ========================================

# LightGBM (GBDT) íŒŒë¼ë¯¸í„°
lgbm_params = {
    "boosting_type": "gbdt",  # Gradient Boosting Decision Tree
    "colsample_bytree": 0.6668813654729736,  # ê° íŠ¸ë¦¬ì—ì„œ ì‚¬ìš©í•  í”¼ì²˜ ë¹„ìœ¨
    "learning_rate": 0.03388752361020876,  # í•™ìŠµë¥ 
    "min_child_samples": 27,  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    "min_child_weight": 0.8201187421311484,  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ê°€ì¤‘ì¹˜
    "n_estimators": 1877,  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜
    "n_jobs": -1,  # ë³‘ë ¬ ì²˜ë¦¬
    "num_leaves": 9,  # íŠ¸ë¦¬ì˜ ìµœëŒ€ ë¦¬í”„ ìˆ˜
    "random_state": 42,
    "reg_alpha": 79.84499181826652,  # L1 ì •ê·œí™”
    "reg_lambda": 49.39626011777799,  # L2 ì •ê·œí™”
    "subsample": 0.2032810514888036,  # ì„œë¸Œìƒ˜í”Œë§ ë¹„ìœ¨
    "verbose": -1
}

# LightGBM (GOSS) íŒŒë¼ë¯¸í„°
lgbm_goss_params = {
    "boosting_type": "goss",  # Gradient-based One-Side Sampling
    "colsample_bytree": 0.36556085663487903,
    "learning_rate": 0.008503127215485715,
    "min_child_samples": 23,
    "min_child_weight": 0.8163027430655353,
    "n_estimators": 601,
    "n_jobs": -1,
    "num_leaves": 68,
    "random_state": 42,
    "reg_alpha": 86.71523829378131,
    "reg_lambda": 9.227695570414218,
    "subsample": 0.4082026328069309,
    "verbose": -1
}

# XGBoost íŒŒë¼ë¯¸í„°
xgb_params = {
    "colsample_bylevel": 0.8640697224146584,  # ë ˆë²¨ë³„ í”¼ì²˜ ìƒ˜í”Œë§
    "colsample_bynode": 0.11087134395654064,  # ë…¸ë“œë³„ í”¼ì²˜ ìƒ˜í”Œë§
    "colsample_bytree": 0.4727045319253941,  # íŠ¸ë¦¬ë³„ í”¼ì²˜ ìƒ˜í”Œë§
    "gamma": 5.886793336088316,  # ìµœì†Œ ë¶„í•  ì†ì‹¤
    "learning_rate": 0.014617031725894066,
    "max_depth": 17,  # ìµœëŒ€ íŠ¸ë¦¬ ê¹Šì´
    "max_leaves": 21,  # ìµœëŒ€ ë¦¬í”„ ìˆ˜
    "min_child_weight": 95,  # ìì‹ ë…¸ë“œì˜ ìµœì†Œ ê°€ì¤‘ì¹˜
    "n_estimators": 761,
    "n_jobs": -1,
    "random_state": 42,
    "reg_alpha": 58.33499071146719,
    "reg_lambda": 59.29921844166945,
    "subsample": 0.05819005276822031,
    "verbosity": 0
}

# ========================================
# 9. ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ ì €ì¥ì†Œ
# ========================================
fold_scores = {}  # ê° í´ë“œë³„ ì ìˆ˜
overall_scores = {}  # ì „ì²´ ì ìˆ˜
oof_preds = {}  # Out-of-fold ì˜ˆì¸¡ê°’
test_preds = {}  # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’

# ========================================
# 10. LightGBM (GBDT) í›ˆë ¨
# ========================================
print("LightGBM (GBDT) ëª¨ë¸ í›ˆë ¨ ì¤‘...")
lgbm_trainer = Trainer(
    LGBMRegressor(**lgbm_params),
    cv=KFold(n_splits=5, shuffle=False),  # 5-í´ë“œ êµì°¨ê²€ì¦
    metric=pearsonr,
    task="regression",
    metric_precision=6
)

lgbm_trainer.fit(X, y)

# ê²°ê³¼ ì €ì¥
fold_scores["LightGBM (gbdt)"] = lgbm_trainer.fold_scores
overall_scores["LightGBM (gbdt)"] = [pearsonr(lgbm_trainer.oof_preds, y)]
oof_preds["LightGBM (gbdt)"] = lgbm_trainer.oof_preds
test_preds["LightGBM (gbdt)"] = lgbm_trainer.predict(X_test)

# ========================================
# 11. LightGBM (GOSS) í›ˆë ¨  
# ========================================
print("LightGBM (GOSS) ëª¨ë¸ í›ˆë ¨ ì¤‘...")
lgbm_goss_trainer = Trainer(
    LGBMRegressor(**lgbm_goss_params),
    cv=KFold(n_splits=5, shuffle=False),
    metric=pearsonr,
    task="regression",
    metric_precision=6
)

lgbm_goss_trainer.fit(X, y)

fold_scores["LightGBM (goss)"] = lgbm_goss_trainer.fold_scores
overall_scores["LightGBM (goss)"] = [pearsonr(lgbm_goss_trainer.oof_preds, y)]
oof_preds["LightGBM (goss)"] = lgbm_goss_trainer.oof_preds
test_preds["LightGBM (goss)"] = lgbm_goss_trainer.predict(X_test)

# ========================================
# 12. XGBoost í›ˆë ¨
# ========================================
print("XGBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...")
xgb_trainer = Trainer(
    XGBRegressor(**xgb_params),
    cv=KFold(n_splits=5, shuffle=False),
    metric=pearsonr,
    task="regression",
    metric_precision=6
)

xgb_trainer.fit(X, y)

fold_scores["XGBoost"] = xgb_trainer.fold_scores
overall_scores["XGBoost"] = [pearsonr(xgb_trainer.oof_preds, y)]
oof_preds["XGBoost"] = xgb_trainer.oof_preds
test_preds["XGBoost"] = xgb_trainer.predict(X_test)

# ========================================
# 13. ì•™ìƒë¸”ì„ ìœ„í•œ Ridge íšŒê·€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
# ========================================

# ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ë“¤ì„ ìƒˆë¡œìš´ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©
X_ensemble = pd.DataFrame(oof_preds)
X_test_ensemble = pd.DataFrame(test_preds)

# ì˜ˆì¸¡ê°’ ì €ì¥ (ì¶”í›„ ì‚¬ìš©ì„ ìœ„í•´)
joblib.dump(X_ensemble, "oof_preds.pkl")
joblib.dump(X_test_ensemble, "test_preds.pkl")

def objective(trial):    
    """Optunaë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ - Ridge íšŒê·€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    params = {
        "random_state": CFG.seed,
        "alpha": trial.suggest_float("alpha", 0, 100),  # L2 ì •ê·œí™” ê°•ë„
        "tol": trial.suggest_float("tol", 1e-6, 1e-2),  # ìˆ˜ë ´ í—ˆìš©ì˜¤ì°¨
        "fit_intercept": trial.suggest_categorical("fit_intercept", [True, False]),  # ì ˆí¸ ì¶”ê°€ ì—¬ë¶€
        "positive": trial.suggest_categorical("positive", [True, False])  # ì–‘ìˆ˜ ì œì•½ ì—¬ë¶€
    }

    trainer = Trainer(
        Ridge(**params),
        cv=KFold(n_splits=5, shuffle=False),
        metric=pearsonr,
        task="regression",
        verbose=False
    )
    trainer.fit(X_ensemble, y)
    
    return pearsonr(trainer.oof_preds, y)

# Optuna ìµœì í™” ì‹¤í–‰
if CFG.run_optuna:
    print("Ridge ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")
    sampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)
    study = optuna.create_study(direction="maximize", sampler=sampler)
    study.optimize(objective, n_trials=CFG.n_optuna_trials, n_jobs=-1, catch=(ValueError,))
    best_params = study.best_params

    ridge_params = {
        "random_state": CFG.seed,
        "alpha": best_params["alpha"],
        "tol": best_params["tol"],
        "fit_intercept": best_params["fit_intercept"],
        "positive": best_params["positive"]
    }
else:
    ridge_params = {"random_state": CFG.seed}

# ========================================
# 14. ìµœì¢… Ridge ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
# ========================================
print("ìµœì¢… Ridge ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
ridge_trainer = Trainer(
    Ridge(**ridge_params),
    cv=KFold(n_splits=5, shuffle=False),
    metric=pearsonr,
    task="regression",
    metric_precision=6
)

ridge_trainer.fit(X_ensemble, y)

fold_scores["Ridge (ensemble)"] = ridge_trainer.fold_scores
overall_scores["Ridge (ensemble)"] = [pearsonr(ridge_trainer.oof_preds, y)]
ridge_test_preds = ridge_trainer.predict(X_test_ensemble)

# ========================================
# 15. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì‹œê°í™” í•¨ìˆ˜
# ========================================
def plot_weights(weights, title):
    """ì•™ìƒë¸” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    sorted_indices = np.argsort(weights[0])[::-1]
    sorted_coeffs = np.array(weights[0])[sorted_indices]
    sorted_model_names = np.array(list(oof_preds.keys()))[sorted_indices]

    plt.figure(figsize=(10, weights.shape[1] * 0.5))
    ax = sns.barplot(x=sorted_coeffs, y=sorted_model_names, palette="RdYlGn_r")

    for i, (value, name) in enumerate(zip(sorted_coeffs, sorted_model_names)):
        if value >= 0:
            ax.text(value, i, f"{value:.3f}", va="center", ha="left", color="black")
        else:
            ax.text(value, i, f"{value:.3f}", va="center", ha="right", color="black")

    xlim = ax.get_xlim()
    ax.set_xlim(xlim[0] - 0.1 * abs(xlim[0]), xlim[1] + 0.1 * abs(xlim[1]))

    plt.title(title)
    plt.xlabel("")
    plt.ylabel("")
    plt.tight_layout()
    plt.show()

# ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì‹œê°í™”
ridge_coeffs = np.zeros((1, X_ensemble.shape[1]))
for m in ridge_trainer.estimators:
    ridge_coeffs += m.coef_
ridge_coeffs = ridge_coeffs / len(ridge_trainer.estimators)

plot_weights(ridge_coeffs, "Ridge ì•™ìƒë¸” ê°€ì¤‘ì¹˜")

# ========================================
# 16. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±
# ========================================
print("ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
sub = pd.read_csv(CFG.sample_sub_path)
sub["prediction"] = ridge_test_preds
sub.to_csv(f"submission_ridge_{overall_scores['Ridge (ensemble)'][0]:.6f}.csv", index=False)

# ========================================
# 17. ê²°ê³¼ ì‹œê°í™”
# ========================================
print("ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
fold_scores_df = pd.DataFrame(fold_scores)
overall_scores_df = pd.DataFrame(overall_scores).transpose().sort_values(by=0, ascending=False)
order = overall_scores_df.index.tolist()

# ê·¸ë˜í”„ ë²”ìœ„ ì„¤ì •
min_score = overall_scores_df.values.flatten().min()
max_score = overall_scores_df.values.flatten().max()
padding = (max_score - min_score) * 0.5
lower_limit = min_score - padding
upper_limit = max_score + padding

# ì‹œê°í™”
fig, axs = plt.subplots(1, 2, figsize=(15, fold_scores_df.shape[1] * 0.5))

# í´ë“œë³„ ì ìˆ˜ ë°•ìŠ¤í”Œë¡¯
boxplot = sns.boxplot(data=fold_scores_df, order=order, ax=axs[0], orient="h", color="grey")
axs[0].set_title("í´ë“œë³„ ì ìˆ˜ ë¶„í¬")
axs[0].set_xlabel("")
axs[0].set_ylabel("")

# ì „ì²´ ì ìˆ˜ ë§‰ëŒ€ê·¸ë˜í”„
barplot = sns.barplot(x=overall_scores_df.values.flatten(), y=overall_scores_df.index, ax=axs[1], color="grey")
axs[1].set_title("ì „ì²´ ì ìˆ˜")
axs[1].set_xlabel("")
axs[1].set_xlim(left=lower_limit, right=upper_limit)
axs[1].set_ylabel("")

# ì•™ìƒë¸” ëª¨ë¸ ê°•ì¡° í‘œì‹œ
for i, (score, model) in enumerate(zip(overall_scores_df.values.flatten(), overall_scores_df.index)):
    color = "cyan" if "ensemble" in model.lower() else "grey"
    barplot.patches[i].set_facecolor(color)
    boxplot.patches[i].set_facecolor(color)
    barplot.text(score, i, round(score, 6), va="center")

plt.tight_layout()
plt.show()

print("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ğŸ† ìµœê³  ì„±ëŠ¥: Ridge ì•™ìƒë¸” - {overall_scores['Ridge (ensemble)'][0]:.6f}") 